---
title: "CogSci Word Counts"
author: "Lauren Oey"
date: "11/24/2018"
output: pdf_document
---

```{r setup, include=FALSE}
library(stringr)
library(dplyr)
library(tidyr)
library(readr)
library(ggplot2)
require(cleanNLP)
require(udpipe)
require(stringi)
library(knitr)
library(GGally)
library(network)
library(sna)
knitr::opts_chunk$set(echo = TRUE)
df <- read_csv("cogsci_papers.csv")
df$abstract <- as.character(df$abstract)
df$full_text <- as.character(df$full_text)
```

# Cleaning up Data Frame

Adds vector of word counts per abstract.
Glimpse at data frame "df" containing a row for each abstract.

```{r}
df$length <- str_count(df$abstract, pattern=" ")+1
glimpse(df)
```

# New Data Frame with a Word for Each Row

Removes punctuation and escape characters, "\\n", "\\t", "\\f".
Creates exception for words containing punctuation, "e.g." & "i.e."
Creates unique row for each word in abstract.
Removes "words" that consist of an empty string.
Creates vector of lower-case version of word.
Writes to new data frame "words".

```{r}
words <- df %>%
  mutate(abstract_cleaned = str_replace_all(abstract, c("e\\.g\\."="e1g1", "i\\.e\\."="i1e1")),
         abstract_cleaned = str_replace_all(abstract_cleaned, c("[^a-zA-Z0-9\\&\\s]"=" ", "[\\n\\t\\f]"="")),
         abstract_cleaned = str_replace_all(abstract_cleaned, c("e1g1"="e.g.", "i1e1"="i.e.")),
         word = strsplit(abstract_cleaned, " ")) %>% 
  unnest(word) %>%
  filter(word != "") %>%
  mutate(lowerword = tolower(word))
```

# Unfiltered Word Frequency Across All Abstracts

40 most frequent words in all abstracts.
As predicted, has a Zipfian distribution.

```{r}
words %>%
  group_by(lowerword) %>%
  summarise(count = n()) %>%
  arrange(desc(count)) %>%
  head(40) %>%
  ggplot(aes(x=reorder(lowerword,count), y=count)) +
  geom_bar(stat="identity") +
  ggtitle("Raw Word Count (All)") +
  scale_x_discrete("Words") +
  coord_flip()
ggsave("graphs/rawWordCount.png")
```

# Including Lemma and Filter by Part-of-Speech (POS) Tagging

Gets lemma & POS info using CleanNLP.
Writes to new data frame "obj_token".
Filters function words (e.g. determiners, prepositions).
Writes to new filtered data frame "obj_token_cleaned".
Joins data about lemma ("obj_token_cleaned") to original words ("words") dataframe.
New data frame "words_full" which also has the function words filtered out.
New vector with lower-cased lemma.

```{r}
## First time running, you need to run this to extract the lemma/POS info from CleanNLP
## This takes awhile, so it's better to save the CSV file and read in the file for future use

## Skip down below to read.csv("token_info_cleaned.csv") to avoid this slow code
cnlp_init_udpipe()
obj <- cnlp_annotate(df$abstract, as_strings = TRUE, doc_ids=df$title)
obj_token <- cnlp_get_token(obj)
write.csv(obj_token, "token_info.csv")
obj_token <- read.csv("token_info.csv")
View(obj_token)

## Filters out most POS, keeping nouns, verbs, adjectives, proper nouns, adverbs, numbers, 
## and INTJ, which is kind of a mix of multiple things

obj_token_cleaned <- obj_token %>%
  filter(upos %in% c("NOUN", "VERB", "ADJ", "PROPN", "ADV", "NUM", "INTJ")) %>%
  mutate(title = id) %>%
  select("title","word","lemma")
  
write.csv(obj_token_cleaned, "token_info_cleaned.csv")

## Skip to here if token_info_cleaned.csv is up to date
token_info_cleaned = read.csv("token_info_cleaned.csv")

words_full <- inner_join(words, unique(token_info_cleaned), by=c("title","word")) %>%
  mutate(lowerlemma = tolower(lemma))
```

# Content Lemma Word Count

Count of content word lemmas.
Visualization of 40 most frequent lemmas across all abstracts.

```{r}
wc_overall <- words_full %>%
  group_by(lowerlemma) %>%
  summarise(count = n()) %>%
  arrange(desc(count))
write.csv(wc_overall, "wc_overall.csv")

wc_overall_read = read.csv("wc_overall.csv")
ggplot(head(wc_overall_read, 40), aes(x=reorder(lowerlemma,count), y=count)) +
  geom_bar(stat="identity") +
  ggtitle("Lemma Count") +
  scale_x_discrete("Lemmas") +
  coord_flip()
ggsave("graphs/overallPopWords.png")
```

# Content Lemma Word Count by Year

Count of content word lemmas by publication year.
Visualization of 10 most frequent lemmas by year.
Fairly consistent across years, mostly contains commonly used verbs (e.g. use, be, show), and words pertaining to the scientific procedure (e.g. participant, study, result, task, model).

```{r}
wc_byYear <- words_full %>%
  group_by(year,lowerlemma) %>%
  summarise(count = n()) %>%
  arrange(year,desc(count))
write.csv(wc_byYear, "wc_byYear.csv")

wc_byYear_read = read.csv("wc_byYear.csv")
wc_byYear_read %>%
  group_by(year) %>%
  top_n(10,count) %>%
  ggplot(aes(x=reorder(lowerlemma,count), y=count)) +
  geom_bar(stat="identity") +
  ggtitle("Lemma Count") +
  scale_x_discrete("Lemmas") +
  coord_flip() +
  facet_wrap(~year, nrow=2, scales="free") +
  theme(axis.text.x=element_text(size=5))
ggsave("graphs/wordsByYear.png")
```

# Content Lemma Word Count by Paper

Count of content lemmas by paper.

```{r}
wc_byTitle <- words_full %>%
  group_by(year,title,lowerlemma) %>%
  summarise(count = n()) %>%
  arrange(year,title,desc(count))
kable(head(wc_byTitle, 20))
write.csv(wc_byTitle, "wc_byTitle.csv")
```

# New Data Frame with an Author for Each Row

Creates new row for each author in $authors vector.
Replaces non-ASCII characters (e.g &uuml; to u); some authors had duplicate names with or without these characters.
Removes final comma in authors list, created during data extraction.
Fixes weird bug where some authors had additional institution information, due to weird formatting in HTML from which data was extracted.
To do this, it removes any text appearing before "\\n" (fix gathered from glancing at the data and seeing this recurring issue).
Fixes weird bug where there are some double white spaces.
Fixes weird bug where numbers appear next to some names, probably indicative of a sub- or superscript in the print.
Replaces "&" with ", ".
Splits authors list by ", " into new rows, so a new row appears for each word corresponding to each author.
Filters out remaining institutions still appearing among authors by removing authors with numbers in their name (indicative of an address) and using a few recurring key words that are unlikely to also be a persons name (e.g. University).
Removes extra white space appearing before and after names.
Creates new factor $authorAbbr tackling issue of authors with multiple names (e.g. names with or without middle initials, nicknames) by extracting last name and first character of first name (e.g. E Vul).
This allows Ed Vul to publish as Edward Vul, Ed Vul, Eddy Vul, Eduardo Vul, E. Vul, Edward Scissorhands Vul, etc. and it will all be categorized as E Vul.
Potential issue 1: Edgar Vul would also be categorized as E Vul, potentially leading to issues if Edward and Edgar Vul are indeed different humans.
Potential issue 2: first name nicknames that differ in the first letter from the full first name appear as different humans when in fact they should be the same author, e.g. Elizabeth Bonawitz = E Bonawitz; Liz Bonawitz = L Bonawitz.
Writes to new data frame "byAuthor".

```{r}
byAuthor <- words_full %>%
  mutate(authors=stri_trans_general(authors, "latin-ascii"),
         author=str_replace_all(authors, ",$", ""),
         author=str_replace_all(author, c(".*\n"="", " *\\(.*?\\)"="", "  "=" ")),
         author=str_replace_all(author, " & ", ", "),
         author=strsplit(author, ", ")) %>%
  unnest(author) %>%
  filter(!grepl("University|Institute|Center|Centre|Centro|School|Department|Dept|Unit|Hospital|\\d",
                author)) %>%
  mutate(author=trimws(author),
         authorAbbr = paste(substring(author,1,1), word(author, -1)))

byAuthor %>%
  filter(grepl("Kim", author)) %>%
  .$author %>%
  unique()

byAuthor %>%
  mutate(lastname = word(author, -1)) %>%
  group_by(lastname) %>%
  select("author", "lastname") %>%
  unique() %>%
  summarise(counts = n()) %>%
  arrange(desc(counts))
```

# Content Lemma Word Count by Author

Count of content lemmas by author.
Visualization of 40 most words used by a single author, visualized as "Author: Lemma".

```{r}
wc_byAuthor <- byAuthor %>%
  group_by(authorAbbr, lowerlemma) %>%
  summarise(count = n()) %>%
  arrange(desc(count),authorAbbr)
write.csv(wc_byAuthor, "wc_byAuthor.csv")

wc_byAuthor %>%
  mutate(authorLemma = paste(authorAbbr, lowerlemma, sep=": ")) %>%
  head(40) %>%
  ggplot(aes(x=reorder(authorLemma,count), y=count)) +
  geom_bar(stat="identity") +
  ggtitle("Author and Lemma Count") +
  scale_x_discrete("Author: Lemma") +
  coord_flip()
ggsave("graphs/AuthorWords.png")
```

# Content Lemma Word Count by Author and Year

Count of content lemmas by author and year.
Allows for looking at trends in author word usage over time.

```{r}
wc_byAuthorYear <- byAuthor %>%
  group_by(year, authorAbbr, lowerlemma) %>%
  summarise(count = n()) %>%
  arrange(year, authorAbbr,desc(count))
write.csv(wc_byAuthorYear, "wc_byAuthorYear.csv")
```

# Leaders in the Field by Most Authored Papers by Year

Counts number of papers published by each author each year.
Selects the top 2 most publishing author for each year.
Creates "leaders" data frame.

```{r}
leaders <- byAuthor %>%
  select("year","authorAbbr","title") %>%
  unique() %>%
  group_by(year, authorAbbr) %>%
  summarise(totalPapers = n()) %>%
  top_n(2, totalPapers)
leaders %>% arrange(year,desc(totalPapers)) %>%
  kable(caption="Top 2 Authors w/ Most Papers by Year")
```

# Authors with Most Words in Abstract by Year

Counts total number of words in abstracts by each author each year.
Selects the top 2 most verbose author for each year.
Approximately corresponds to top 3 authors by number of papers published (i.e. authors who have more papers correspondingly have more words).

```{r}
byAuthor %>%
  group_by(year, authorAbbr) %>%
  summarise(total = n()) %>%
  top_n(2, total) %>%
  arrange(year, desc(total)) %>%
  kable(caption="Top 2 Authors w/ Most Abstract Words by Year")
```

# Favorite Words of Field Leaders and Trends

Extracts the 5 most frequent words used by the leaders determined in the "leaders" data frame.
Visualizes the frequency of word usages for each author over time.

```{r}
leaderFaveWords <- byAuthor %>%
  filter(authorAbbr %in% leaders$authorAbbr) %>%
  group_by(authorAbbr, lowerlemma) %>%
  summarise(count = n()) %>%
  top_n(5, count) %>%
  mutate(authorWord = paste(authorAbbr, lowerlemma, sep="_")) %>%
  arrange(authorAbbr, desc(count))

wc_byAuthorYear %>%
  filter(authorAbbr %in% unique(leaderFaveWords$authorAbbr)) %>%
  mutate(authorWord=paste(authorAbbr, lowerlemma, sep="_")) %>%
  filter(authorWord %in% unique(leaderFaveWords$authorWord)) %>%
  ggplot(aes(x=year, y=count, colour=lowerlemma)) +
  geom_line(stat="identity") +
  ggtitle("Trends in Leader's Most Popular Words") +
  facet_wrap(~authorAbbr)
ggsave("graphs/leaderPopWords.png")
```

```{r}
X <- .01 #top percent of authors we'll examine
mostPublished <- byAuthor %>%
  select("year", "title", "authorAbbr") %>%
  unique() %>%
  group_by(authorAbbr) %>%
  summarise(papersCount = n()) %>%
  top_n(ceiling(X*nrow(.)), papersCount) %>%
  arrange(desc(papersCount))

topXpercent <- nrow(mostPublished) #96 people in network

authorPapers <- byAuthor %>%
  select("year", "title", "authorAbbr") %>%
  unique() %>%
  filter(authorAbbr %in% mostPublished$authorAbbr)

matrixArr = c()
for(a in mostPublished$authorAbbr){
  df.a <- authorPapers %>%
    filter(authorAbbr == a)
  for(b in mostPublished$authorAbbr){
    df.b <- authorPapers %>%
      filter(authorAbbr == b)
    matrixArr <- c(matrixArr, length(intersect(df.a$title, df.b$title)))
  }
}

matr <- matrix(matrixArr, nrow=topXpercent, dimnames = list(mostPublished$authorAbbr, mostPublished$authorAbbr))

net <- network(matr, directed = FALSE)
network.vertex.names(net) = mostPublished$authorAbbr

ggnet2(net, node.size = 2, node.color="black")

ggsave("graphs/network_top1percent.png")

```




