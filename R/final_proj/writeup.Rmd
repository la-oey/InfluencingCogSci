---
title: "Author Topic Space Dynamics"
author: Erik Brockbank, Isabella DeStefano, Lauren A. Oey, Hayden Schill, Jamal Williams
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=FALSE, warning=FALSE}
library(tidyverse)
source("networkCentralityFunctions.R")
df <- read_csv("topic_dist_fulltext_100.csv")
authorCounts <- read_csv("author_net.csv")
```


# Introduction #



# Data #

We developed a novel corpus of CogSci papers that consists of the title, authors, abstracts, full text, and year of conference papers from 2000 to 2018. Original texts were hosted by two websites: eScholarship and MindModeling. To extract the data from the web, we used a number of Python libraries, namely the web scraping library Beautiful Soup (bs4), a PDF-to-text converter (pdfminer), a URL handling module (urllib), and a regular expressions handling library (re). After data extraction, we processed the raw text in R. This involved removing irrelevant text (e.g. cover pages, text appearing before the abstract, etc.), removing punctuation, non-ASCII characters, escape characters, and other text cleaning tasks. In our final analysis, we excluded the 13.70% of the papers for which we were missing the corresponding full text. Our final data set included 9,165 papers and 11,312 unique authors.

Our analysis required careful mapping of research papers to their corresponding authors, yet collecting data from multiple websites with different formatting led to lack of systematicity in author names. One issue was that there were multiple name entries for the same author (e.g. Joshua Tenenbaum, Joshua B. Tenenbaum, Josh Tenenbaum). Another related issue, which we have dubbed the "*Kim quandary*", is that an authorâ€™s last name often failed to act as a unique identifier. This was especially problematic for popular surnames used by a plethora of authors (e.g. Kim, Smith, Williams). To cope with these issues we resorted to identifying authors by last name and first initial (e.g. J Tenenbaum, E Vul), which although not perfect, certainly reduces the impact of the *Kim quandary*.

The impact of CogSci has grown over the last few years and Figure 1 demonstrates this general upward trend. Furthermore, there are several leading authors in the field. The publications of these leaders increase over the years as visualized in Figure 2.

```{r papersByYear, message=FALSE, warning=FALSE, fig.cap="**Fig. 1: The count of CogSci papers by year (removing papers missing the full text).**"}
df %>%
  group_by(year) %>%
  summarise(count = n()) %>%
  ggplot(aes(x=year, y=count)) +
  geom_line() +
  scale_x_continuous(limits=c(1999,2020)) +
  scale_y_continuous("count of papers") +
  theme_minimal()
```

```{r leadAuths, message=FALSE, warning=FALSE, fig.cap="**Fig. 2: The top 6 most published authors (and our sovereign leader, Sir E Vul) and their proportion of contribution out of all CogSci papers contributed for a given year.**"}
byAuth <- transform_authorAbbr(df) %>%
  mutate(author=strsplit(authors, ", ")) %>%
  unnest(author) %>%
  select(author, title, year)

topAuth <- authorCounts %>%
  top_n(6,n) %>%
  .$authorAbbr

byAuth %>%
  group_by(year, author) %>%
  summarise(count = n()) %>%
  ungroup() %>%
  group_by(year) %>%
  mutate(prop = count/n()) %>%
  filter(author %in% topAuth | author == "E Vul") %>%
  ggplot(aes(x=year, y=prop, colour=author)) +
  geom_smooth(se=F) +
  scale_x_continuous(limits=c(1999,2020)) +
  scale_y_continuous("proportion of papers") +
  theme_minimal()
```





# Methods #


### Author Network Centrality ###

```{r}
topic.df.50 = read_csv(DATA_50) %>% transform_authorAbbr()

author_net = getAuthorList(topic.df.50)

centrality = writeAuthorNet(author_net)

```


```{r centrality grpah  message=FALSE, warning=FALSE, fig.cap="**Fig. ??: Degree centrality has a long tail, and so it must be log transformed for our analysis."}
centrality %>% 
  filter(CM == 'degree') %>% 
  arrange(desc(measure)) %>%
  select(-id,-CM) %>%
  mutate(rank = dense_rank(desc(measure)))%>%
  ggplot(aes(x=reorder(factor(label),rank),y=measure))+
  geom_bar(stat='identity')+
  xlab('Author (ordered by centrality)')+
  ylab('centrality vlaue' )
  theme(axis.text.x = element_blank())
```

### Topic Model ###


### Field Dynamics ###

```{r get centrality subset}
author_net_filtered = author_net %>%
  group_by(authorAbbr) %>%
  summarise(n = n()) %>%
  filter(n > 1) %>%
  ungroup()

centrality_subset = centralityQuantile(centrality, q = c(0,1)) %>% filter(label %in% author_net_filtered$authorAbbr)
```


```{r create sparse matricies}
author.mat = Matrix(0, 
                    nrow = length(papers),
                    ncol = length(authors),
                    dimnames = list(papers, as.character(authors)))

for (col in seq(1:dim(author.mat)[2])) { # This takes 3 mins
  author.lookup = colnames(author.mat)[col]
  author.grep = as.vector(unlist(sapply(author.lookup, function(author.lookup){return(grep(author.lookup, topic.df.50$authors, value = TRUE))})))
  author.rows = topic.df.50 %>%
    filter(authors %in% author.grep) %>%
    select(title)
  if (sum(rownames(author.mat) %in% author.rows$title) > 0) {
    author.mat[rownames(author.mat) %in% author.rows$title, colnames(author.mat)[col]] = 1
  }
}


year.mat = Matrix(0,
                  nrow = length(papers),
                  ncol = length(years),
                  dimnames = list(papers, as.character(years)))

for (col in seq(1:dim(year.mat)[2])) { # This takes ~1s
  year.lookup = colnames(year.mat)[col]
  year.rows = topic.df.50 %>%
    filter(as.character(year) == year.lookup) %>%
    select(title)
  year.mat[rownames(year.mat) %in% year.rows$title, colnames(year.mat)[col]] = 1
}
```


```{r calculate dependent measure}

global_topic.50 =  topic.df.50 %>% 
  globalTopicDistByYear(., 50) %>% # long format with topic, year, probability (global average of each topic by year)
  group_by(topic) %>% 
  mutate(diff = lead(prob,3) - prob)

author_influence.50 = centrality_subset %>%
  pull(label) %>%
  unique() %>%
  mapply(authorsInfluence, ., MoreArgs = list(topic.df = topic.df.50, N = 50,
                                              author.matrix = author.mat, year.matrix = year.mat,
                                              global_topic = global_topic.50))
```

# Results #

```{r Calculate cor}
author_influence.cors = data.frame(t(author_influence.50))%>%
  inner_join(centrality_subset, by = c("author" = "label"))%>%
  filter(global_influence_author != 'NaN', author_influence_global != 'NaN')%>%
  mutate(author_influence_global = as.numeric(as.character(author_influence_global)),
         global_influence_author = as.numeric(as.character(global_influence_author)))%>%
  do(cor.global.author = cor.test(.$global_influence_author,.$author_influence_global),
            cor.global.influence.author.centrality = cor.test(.$global_influence_author,log(.$measure)),
            cor.author.influence.global.centrality = cor.test(.$author_influence_global,log(.$measure)))
```


```{r cor test results}
author_influence.cors$cor.global.influence.author


author_influence.cors$cor.author.influence.global
```


```{r Null randomization}
indx = sample(1:nrow(author.matrix),nrow(author.matrix))
author.matrix = author.matrix[indx,]
```

```{r NHST}
H0 = read_csv('H0.csv')

cor_val_gia = author_influence.cors$cor.global.influence.author[[1]]$estimate
cor_val_aig = author_influence.cors$cor.author.influence.global[[1]]$estimate

pval_giac = (sum(H0$cor.GIAC.cor>cor_val_gia)+1)/(length(H0$cor.GIAC.cor)+2)
pval_giac*2

pval_aigc = (sum(H0$cor.AIGC.cor>cor_val_aig)+1)/(length(H0$cor.AIGC.cor)+2)
pval_aigc = (1-pval_aigc)*2

```


```{r NHST figures}
H0 %>% 
  ggplot(aes(x=cor.GIAC.cor))+
  geom_histogram()+
  geom_vline(xintercept = cor_val_gai)+
  scale_y_continuous(expand = c(0,0))+
  theme_minimal()+
  xlab('r')+
  ggtitle('Null hypothesis for Global influence Author')


H0 %>% 
  ggplot(aes(x=cor.AIGC.cor))+
  geom_histogram()+
  geom_vline(xintercept = cor_val_aig)+
  scale_y_continuous(expand = c(0,0))+
  theme_minimal()+
  xlab('r')+
  ggtitle('Null hypothesis for Author influence Global')
```
# Discussion #






