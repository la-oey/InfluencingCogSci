max.em.its = 75,
init.type = "Spectral") # Takes 1-2 mins.
# Validate model
labelTopics(abstract.model.manual)
library(wordcloud)
cloud(stmobj = abstract.model.manual,
topic = 1,
type = "model",
max.words = 25) # word cloud of most probable 25 words in topic 1
cloud(stmobj = abstract.model.manual,
topic = 1,
type = "documents",
documents = abstract.model.framework$documents,
thresh = 0.8,
max.words = 25) # word cloud of most probable 25 words in topic 1 selected from most likely documents
# View the top n documents for each topic
# TODO get this working: how to figure out which abstracts were removed during processing
topic.docs <- findThoughts(abstract.model, texts = new.docs, n = 3, topics = c(1))
# View the top n documents for each topic
# TODO get this working: how to figure out which abstracts were removed during processing
topic.docs <- findThoughts(abstract.model.manual, texts = new.docs, n = 3, topics = c(1))
# View the top n documents for each topic
# TODO get this working: how to figure out which abstracts were removed during processing
topic.docs <- findThoughts(abstract.model.manual, texts = new.docs$abstract_cleaned, n = 3, topics = c(1))
topic.docs
length(topics.docs)
length(topic.docs)
dim(topic.docs)
glimpse(topic.docs)
topic.docs[1]
topic.docs[2]
rm(list=ls())
# Model full text
df.fulltext <- read_csv(DATA)
DATA = "cogsci_papers.csv"
# Model full text
df.fulltext <- read_csv(DATA)
#  TODO why does metadata not work?
fulltext.model.framework <- structure_text(df.fulltext$full_text) # takes up to 20 mins.
structure_text <- function(documents, metadata = NA) {
print("Processing documents")
if (!is.na(metadata)) {
processed <- textProcessor(documents, metadata = metadata)
} else {
processed <- textProcessor(documents)
}
print("Preparing documents for modeling")
out <- prepDocuments(processed$documents, processed$vocab, processed$meta, lower.thresh = 10)
return(out)
}
#  TODO why does metadata not work?
fulltext.model.framework <- structure_text(df.fulltext$full_text) # takes up to 20 mins.
# Fit model
fulltext.model.manual <- stm(documents = fulltext.model.framework$documents,
vocab = fulltext.model.framework$vocab,
K = 50,
max.em.its = 100,
init.type = "Spectral") # Takes up to 10 mins.
# Validate model
labelTopics(fulltext.model.manual)
# Graph model
cloud(stmobj = fulltext.model.manual,
topic = 2,
type = "model",
max.words = 25) # word cloud of most probable 25 words in topic 1
# Graph model
cloud(stmobj = fulltext.model.manual,
topic = 45,
type = "model",
max.words = 25) # word cloud of most probable 25 words in topic 1
# Graph model
cloud(stmobj = fulltext.model.manual,
topic = 37,
type = "model",
max.words = 25) # word cloud of most probable 25 words in topic 1
# Graph model
cloud(stmobj = fulltext.model.manual,
topic = 22,
type = "model",
max.words = 25) # word cloud of most probable 25 words in topic 1
getwd()
DATA = "cogsci_papers.csv"
DATA_ALT = "cogsci_abstracts.csv"
# Model original abstracts data
df.abstracts.alt <- read_csv(DATA_ALT)
library(stm)
library(stringr)
library(dplyr)
library(tidyr)
library(readr)
library(ggplot2)
require(cleanNLP)
require(udpipe)
require(stringi)
library(knitr)
library(GGally)
library(network)
library(sna)
library(wordcloud)
clean_abstracts <- function(data_frame) {
# Clean abstract column (expects a data frame with an `abstract` column)
# Removes punctuation and escape characters, "\\n", "\\t", "\\f".
# Creates exception for words containing punctuation, "e.g." & "i.e."
data_frame$abstract <- as.character(data_frame$abstract, na.omit = T)
data_frame <- data_frame %>%
mutate(abstract_cleaned = str_replace_all(abstract, c("e\\.g\\."="e1g1", "i\\.e\\."="i1e1")),
abstract_cleaned = str_replace_all(abstract_cleaned, c("[^a-zA-Z0-9\\&\\s]"=" ", "[\\n\\t\\f]"="")),
abstract_cleaned = str_replace_all(abstract_cleaned, c("e1g1"="e.g.", "i1e1"="i.e.")))
return(data_frame)
}
structure_text <- function(documents, metadata = NA) {
print("Processing documents")
if (!is.na(metadata)) {
processed <- textProcessor(documents, metadata = metadata)
} else {
processed <- textProcessor(documents)
}
print("Preparing documents for modeling")
out <- prepDocuments(processed$documents, processed$vocab, processed$meta, lower.thresh = 10)
return(out)
}
DATA = "cogsci_papers.csv"
DATA_ALT = "cogsci_abstracts.csv"
# Model original abstracts data
df.abstracts.alt <- read_csv(DATA_ALT)
# Model full text
df.fulltext <- read_csv(DATA)
glimpse(df.fulltext)
df.fulltext %>%
group_by(year) %>%
summarize(papers = n())
2018.papers = df.fulltext %>%
2018.papers = df.fulltext %>%
2018.papers <- df.fulltext %>%
# TODO parse these out by year? ...
2018.papers <- df.fulltext %>%
filter(year == '2018')
# TODO parse these out by year? ...
papers.2018 <- df.fulltext %>%
filter(year == '2018')
glimpse(papers.2018)
# TODO parse these out by year? ...
papers.2017 <- df.fulltext %>%
filter(year == '2017')
glimpse(papers.2017)
fulltext.model.framework.2017 <- structure_text(papers.2017$full_text)
fulltext.model.framework.2017$docs.removed
# Summary of fulltext.model.framework.2017:
# 885 documents, 4110 terms and 365292 tokens
fulltext.model.manual.2017 <- stm(documents = fulltext.model.framework.2017$documents,
vocab = fulltext.model.framework.2017$vocab,
K = 20,
max.em.its = 100,
init.type = "Spectral")
# validation
labelTopics(fulltext.model.manual.2017)
summary.STM(fulltext.model.manual.2017)
summary(fulltext.model.manual.2017)
fulltext.model.manual.2017$theta
topic.dist = fulltext.model.manual.2017$theta
dims(topics.dist)
dims(topic.dist)
topic.dist.dims
lenght(topic.dist)
topic.dist.length
len(topic.dist)
length(topic.dist)
dim(topic.dist)
topic.dist[1,]
dim(topic.dist) ;topic.dist[1,]
names(topic.dist)
fulltext.model.framework.2017$documents
dim(fulltext.model.framework.2017$documents)
len(fulltext.model.framework.2017$documents)
length(fulltext.model.framework.2017$documents)
fulltext.model.framework.2017$documents[1]
names(fulltext.model.framework.2017$documents)
fulltext.model.framework.2017$docs.removed
get_removed_docs <- function(documents) {
processed <- textProcessor(documents)
removed = processed$docs.removed
return(removed)
}
removed.docs.2017 = get_removed_docs(papers.2017$full_text)
glimpse(papers.2017)
glimpse(papers.2017$full_text)
glimpse(removed.docs.2017)
rm(list=ls())
clean_abstracts <- function(data_frame) {
# Clean abstract column (expects a data frame with an `abstract` column)
# Removes punctuation and escape characters, "\\n", "\\t", "\\f".
# Creates exception for words containing punctuation, "e.g." & "i.e."
data_frame$abstract <- as.character(data_frame$abstract, na.omit = T)
data_frame <- data_frame %>%
mutate(abstract_cleaned = str_replace_all(abstract, c("e\\.g\\."="e1g1", "i\\.e\\."="i1e1")),
abstract_cleaned = str_replace_all(abstract_cleaned, c("[^a-zA-Z0-9\\&\\s]"=" ", "[\\n\\t\\f]"="")),
abstract_cleaned = str_replace_all(abstract_cleaned, c("e1g1"="e.g.", "i1e1"="i.e.")))
return(data_frame)
}
structure_text <- function(documents, metadata = NA) {
print("Processing documents")
if (!is.na(metadata)) {
processed <- textProcessor(documents, metadata = metadata)
} else {
processed <- textProcessor(documents)
}
print("Preparing documents for modeling")
out <- prepDocuments(processed$documents, processed$vocab, processed$meta, lower.thresh = 10)
return(out)
}
get_removed_docs <- function(documents) {
processed <- textProcessor(documents)
removed = processed$docs.removed
return(removed)
}
DATA = "cogsci_papers.csv"
# Model full text
df.fulltext <- read_csv(DATA)
# Try modeling individual years
papers.2017 <- df.fulltext %>%
filter(year == '2017')
removed.docs.2017 = get_removed_docs(papers.2017$full_text)
docs.2017.cleaned = papers.2017[-removed.docs.2017]
docs.2017.cleaned = papers.2017[-removed.docs.2017,]
glimpse(docs.2017.cleaned)
fulltext.model.framework.2017 <- structure_text(papers.2017$full_text)
# Summary of fulltext.model.framework.2017:
# 885 documents, 4110 terms and 365292 tokens
fulltext.model.manual.2017 <- stm(documents = fulltext.model.framework.2017$documents,
vocab = fulltext.model.framework.2017$vocab,
K = 20, # converges after 82 iters for k = 20, very fast
max.em.its = 100,
init.type = "Spectral")
# analysis
topic.dist = fulltext.model.manual.2017$theta
dim(topic.dist) ;topic.dist[1,]
df.topic.dist = data.frame(title = docs.2017.cleaned$title,
authors = docs.2017.cleaned$authors,
year = docs.2017.cleaned$year)
glimpse(df.topic.dist)
?cbind
tmp = cbind(df.topic.dist, topic.dist)
glimspe(tmp)
glimpse(tmp)
df.papers = data.frame(title = docs.2017.cleaned$title,
authors = docs.2017.cleaned$authors,
year = docs.2017.cleaned$year)
df.topic.dist = cbind(df.papers, topic.dist)
df.topic.dist = cbind(df.papers, topic.dist)
# Model fulltext from individual years
sample.year = '2017'
papers.sample.year <- df.fulltext %>%
filter(year == sample.year)
removed.docs.sample.year = get_removed_docs(papers.sample.year$full_text)
docs.sample.year.cleaned = papers.sample.year[-removed.docs.sample.year,]
fulltext.model.framework.sample.year <- structure_text(papers.sample.year$full_text)
# Summary of fulltext.model.framework.2017:
# 885 documents, 4110 terms and 365292 tokens
fulltext.model.manual.sample.year <- stm(documents = fulltext.model.framework.sample.year$documents,
vocab = fulltext.model.framework.sample.year$vocab,
K = 20, # converges after 82 iters for k = 20, very fast
max.em.its = 100,
init.type = "Spectral")
# validation
summary(fulltext.model.manual.sample.year)  # looks pretty good
# analysis
topic.dist = fulltext.model.manual.sample.year$theta
dim(topic.dist) ;topic.dist[1,]
sum(topic.dist[1,])
df.papers = data.frame(title = docs.sample.year.cleaned$title,
authors = docs.sample.year.cleaned$authors,
year = docs.sample.year.cleaned$year)
df.topic.dist = cbind(df.papers, topic.dist)
glimpse(df.topic.dist)
max(topic.dist[1,3:])
max(topic.dist[1,])
max(df.papers[1,])
df.papers[1,]
df.topic.dist[1,]
max(df.topic.dist[1,])
write_csv(df.topic.dist, 'topic_dist_year.csv')
topic.df = read_csv(DATA)
glimpse(topic.df)
rm(list=ls())
DATA = 'topic_dist_year.csv'
topic.df = read_csv(DATA)
glimpse(topic.df)
# get mean topic distribution for a set of documents
get_avg_topic_dist <- function(df) {
topics.only = df[-df$author]
}
glimpse(topics.only)
# get mean topic distribution for a set of documents
get_avg_topic_dist <- function(df) {
topics.only = df[-df$author]
glimpse(topics.only)
}
DATA = 'topic_dist_year.csv'
topic.df = read_csv(DATA)
get_avg_topic_dist(topic.df)
df[,3:]
df[,3]
topic.df[,3]
topic.df[,4]
topic.df[,4:]
topic.df[-topic.df$authors]
topic.df[-1]
tmp = topic.df %>%
select(-title, -author, -year)
tmp = topic.df %>%
select(-title, -authors, -year)
glimpse(tmp)
dim(tmp)
dim(tmp)[1]
dim(tmp)[2]
seq(1, dim(tmp)[2])
names(tmp)
means = data.frame(names(tmp))
means
numtopics = dim(topics.only)[2]
topics.only = df %>%
select(-title, -authors, -year)
df = topic.df
topics.only = df %>%
select(-title, -authors, -year)
numtopics = dim(topics.only)[2]
vector.means = data.frame(names(topics.only))
glimpse(topics.only)
numtoipcs
numtopics
vector.means
for (col in seq(1, numtopics)) {
topic.data = topics.only[,col]
topic.mean = mean(topic.data)
tmp = c(tmp, topic.mean) # TODO there has to be a more efficient way to do this...
}
warnings()
tmp
col = 3
data = topics.only[,col]
glimpse(data)
mean(data)
mean(data, na.rm=T)
mean(data, na.rm=TRUE)
colMeans(topics.only)
colMeans(na.omit(topics.only))
colMeans(topics.only) - colMeans(na.omit(topics.only))
colMeans(topic.df)
topic.means = df %>%
select(-title, -authors, -year) %>%
colMeans()
topic.means
topic.df = read_csv(DATA)
global.means = get_avg_topic_dist(topic.df)
get_avg_topic_dist <- function(df) {
topic.means = df %>%
select(-title, -authors, -year) %>%
colMeans()
}
global.means = get_avg_topic_dist(topic.df)
global.means
glimpse(topic.df)
paper.topics = topic.df[1,]
paper.topics
glimpse(paper.topics)
sample.paper.topics = topic.df[1,]
sample.paper.topic.dist = sample.paper.topics %>%
select(-title, -authors, -year)
glimpse(sample.paper.topic.dist)
glimpse(global.means)
dim(global.means)
global.means = as.data.frame(global.means)
global.means
sample.paper.topic.dist
sample.paper.topic.dist = as.data.frame(sample.paper.topic.dist)
sample.paper.topic.dist
global.means / sample.paper.topic.dist
dim(global.means)
dim(sample.paper.topic.dist)
rbind(c(sample.paper.topic.dist))
dim(rbind(c(sample.paper.topic.dist)))
dim(cbind(c(sample.paper.topic.dist)))
paper.topic.dist = cbind(c(sample.paper.topic.dist)) # convert to 1 col, X rows
dim(paper.topic.dist)
paper.topic.dist
global.means / paper.topic.dist
global.means * paper.topic.dist
paper.topic.dist = data.frame(topicMeans = paper.topic.dist)
paper.topic.dist
global.means / paper.topic.dist
global.means
global.means$global.means / paper.topic.dist$topicMeans
global.means$global.means
paper.topic.dist$topicMeans
paper.topic.dist = sample.paper %>%
select(-title, -authors, -year)
sample.paper.topics = topic.df[1,]
paper.topic.dist = sample.paper %>%
select(-title, -authors, -year)
paper.topic.dist = sample.paper.topics %>%
select(-title, -authors, -year)
paper.topic.dist
library(tibble)
tmp = paper.topic.dist %>%
column_to_rownames("topicMeans")
glimpse(paper.topic.dist)
dim(paper.topic.dist)
paper.topic.dist %>%
gather(topic, topicMean, `1`:`20`)
paper.topic.dist = sample.paper.topics %>%
select(-title, -authors, -year) %>%
gather(topic, topicMean, `1`:`20`)
glimpse(paper.topic.dist)
dim(paper.topic.dist)
dim(global.means)
global.means$global.means / paper.topic.dist$topicMean
global.means / paper.topic.dist$topicMean
data.frame(topicMultiplier = global.means$global.means / paper.topic.dist$topicMean)
get_paper_global_comparison <- function(sample.paper.topics, global.topic.dist) {
paper.topic.dist = sample.paper.topics %>%
select(-title, -authors, -year) %>%
gather(topic, topicMean, `1`:`20`)
paper.topic.multiplier = data.frame(topicMultiplier = global.topic.dist$global.means / paper.topic.dist$topicMean)
}
rm(list=ls())
get_avg_topic_dist <- function(df) {
topic.means = df %>%
select(-title, -authors, -year) %>%
colMeans()
topic.means = as.data.frame(topic.means)
}
get_paper_global_comparison <- function(sample.paper.topics, global.topic.dist) {
paper.topic.dist = sample.paper.topics %>%
select(-title, -authors, -year) %>%
gather(topic, topicMean, `1`:`20`)
paper.topic.multiplier = data.frame(topicMultiplier = global.topic.dist$global.means / paper.topic.dist$topicMean)
}
DATA = 'topic_dist_year.csv'
topic.df = read_csv(DATA)
global.means = get_avg_topic_dist(topic.df)
glimpse(global.means)
sample.paper.topics = topic.df[1,]
glimpse(sample.paper.topics)
sample.paper.multiplier = get_paper_global_comparison(sample.paper.topics, global.means)
glimpse_sample.paper.multiplier
glimpse(sample.paper.multiplier)
get_paper_global_comparison <- function(sample.paper.topics, global.topic.dist) {
paper.topic.dist = sample.paper.topics %>%
select(-title, -authors, -year) %>%
gather(topic, topicMean, `1`:`20`)
paper.topic.multiplier = global.topic.dist$global.means / paper.topic.dist$topicMean
}
sample.paper.multiplier = get_paper_global_comparison(sample.paper.topics, global.means)
sample.paper.multiplier
paper.topic.dist = sample.paper.topics %>%
select(-title, -authors, -year) %>%
gather(topic, topicMean, `1`:`20`)
paper.topic.dist
global.topic.dist$global.means / paper.topic.dist$topicMean
global.topic.dist = global.means
global.topic.dist$global.means / paper.topic.dist$topicMean
global.topic.dist$global.means
global.topic.dist
get_paper_global_comparison <- function(sample.paper.topics, global.topic.dist) {
paper.topic.dist = sample.paper.topics %>%
select(-title, -authors, -year) %>%
gather(topic, topicMean, `1`:`20`)
paper.topic.multiplier = global.topic.dist$topic.means / paper.topic.dist$topicMean
}
sample.paper.multiplier = get_paper_global_comparison(sample.paper.topics, global.means)
sample.paper.multiplier
get_paper_global_comparison <- function(sample.paper.topics, global.topic.dist) {
paper.topic.dist = sample.paper.topics %>%
select(-title, -authors, -year) %>%
gather(topic, topicMean, `1`:`20`)
paper.topic.multiplier = data.frame(topicMult = global.topic.dist$topic.means / paper.topic.dist$topicMean)
}
sample.paper.multiplier = get_paper_global_comparison(sample.paper.topics, global.means)
sample.paper.multiplier
rm(list=ls())
get_avg_topic_dist <- function(df) {
topic.means = df %>%
select(-title, -authors, -year) %>%
colMeans()
topic.means = as.data.frame(topic.means)
}
get_paper_global_comparison <- function(sample.paper.topics, global.topic.dist) {
paper.topic.dist = sample.paper.topics %>%
select(-title, -authors, -year) %>%
gather(topic, topicMean, `1`:`20`)
paper.topic.multiplier = data.frame(topicMult = global.topic.dist$topic.means / paper.topic.dist$topicMean)
}
DATA = 'topic_dist_year.csv'
topic.df = read_csv(DATA)
global.means = get_avg_topic_dist(topic.df)
sample.paper.topics = topic.df[1,]
sample.paper.multiplier = get_paper_global_comparison(sample.paper.topics, global.means)
sample.paper.multiplier
paper.topic.dist = sample.paper.topics %>%
select(-title, -authors, -year) %>%
gather(topic, topicMean, `1`:`20`)
global.means$topic.means == paper.topic.dist$topicMean * sample.paper.multiplier$topicMult
global.means
paper.topic.dist$topicMean * sample.paper.multiplier$topicMult
get_paper_global_comparison <- function(sample.paper.topics, global.topic.dist) {
paper.topic.dist = sample.paper.topics %>%
select(-title, -authors, -year) %>%
gather(topic, topicMean, `1`:`20`)
return(data.frame(topicMult = global.topic.dist$topic.means / paper.topic.dist$topicMean))
}
sample.paper.multiplier = get_paper_global_comparison(sample.paper.topics, global.means)
sample.paper.multiplier
