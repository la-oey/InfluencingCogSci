# abstract.model.framework <- model_abstracts()
abstract.model.framework <- model_abstracts("cogsci_papers.csv")
df <- read_csv("cogsci_papers.csv") # takes < 10s; NB: read.csv takes ~ 3 mins
glimpse(df)
glimpse(df$abstract)
levels(df$abstract)
summary(df$abstract)
clean_abstracts <- function(data_frame) {
# Clean abstract column
# Removes punctuation and escape characters, "\\n", "\\t", "\\f".
# Creates exception for words containing punctuation, "e.g." & "i.e."
data_frame$abstract <- as.character(df$abstract, na.omit = T)
# data_frame <- data_frame %>%
#   mutate(abstract_cleaned = str_replace_all(abstract, c("e\\.g\\."="e1g1", "i\\.e\\."="i1e1")),
#          abstract_cleaned = str_replace_all(abstract_cleaned, c("[^a-zA-Z0-9\\&\\s]"=" ", "[\\n\\t\\f]"="")),
#          abstract_cleaned = str_replace_all(abstract_cleaned, c("e1g1"="e.g.", "i1e1"="i.e.")))
}
# abstract.model.framework <- model_abstracts()
abstract.model.framework <- model_abstracts("cogsci_papers.csv")
clean_abstracts <- function(data_frame) {
# Clean abstract column
# Removes punctuation and escape characters, "\\n", "\\t", "\\f".
# Creates exception for words containing punctuation, "e.g." & "i.e."
data_frame$abstract <- as.character(df$abstract, na.omit = T)
# data_frame <- data_frame %>%
#   mutate(abstract_cleaned = str_replace_all(abstract, c("e\\.g\\."="e1g1", "i\\.e\\."="i1e1")),
#          abstract_cleaned = str_replace_all(abstract_cleaned, c("[^a-zA-Z0-9\\&\\s]"=" ", "[\\n\\t\\f]"="")),
#          abstract_cleaned = str_replace_all(abstract_cleaned, c("e1g1"="e.g.", "i1e1"="i.e.")))
return(data_frame)
}
# abstract.model.framework <- model_abstracts()
abstract.model.framework <- model_abstracts("cogsci_papers.csv")
clean_abstracts <- function(data_frame) {
# Clean abstract column
# Removes punctuation and escape characters, "\\n", "\\t", "\\f".
# Creates exception for words containing punctuation, "e.g." & "i.e."
data_frame$abstract <- as.character(df$abstract, na.omit = T)
data_frame <- data_frame %>%
mutate(abstract_cleaned = str_replace_all(abstract, c("e\\.g\\."="e1g1", "i\\.e\\."="i1e1")),
abstract_cleaned = str_replace_all(abstract_cleaned, c("[^a-zA-Z0-9\\&\\s]"=" ", "[\\n\\t\\f]"="")),
abstract_cleaned = str_replace_all(abstract_cleaned, c("e1g1"="e.g.", "i1e1"="i.e.")))
return(data_frame)
}
# abstract.model.framework <- model_abstracts()
abstract.model.framework <- model_abstracts("cogsci_papers.csv")
model_abstracts <- function(documents, metadata) {
# apply author clean-up that Lauren does in initial data cleaning then include author in metadata
print("Processing abstracts")
processed <- textProcessor(documents, metadata = metadata)
# TODO experiment with `lower.thresh` arg to prepDocuments
# plotRemoved(processed$documents, lower.thresh = seq(1, 200, by = 10))
print("Preparing documents for modeling")
out <- prepDocuments(processed$documents, processed$vocab, processed$meta, lower.thresh = 10)
# TODO what kind of additional processing needs doing to sanity check the above?
# (e.g. seeing which docs were removed entirely)
return(out)
}
# abstract.model.framework <- model_abstracts()
DATA = "cogsci_papers.csv"
print("Reading in abstracts data")
df <- read_csv(DATA)
print("Cleaning abstracts")
df <- clean_abstracts(df)
print("Generating model framework")
abstract.model.framework <- model_abstracts(df$abstract_cleaned, df)
# Sanity checks
length(abstract.model.framework$documents) # 7844 compare to 7871 from initial read.csv
length(abstract.model.framework$vocab)
length(abstract.model.framework$meta)
clean_abstracts <- function(data_frame) {
# Clean abstract column
# Removes punctuation and escape characters, "\\n", "\\t", "\\f".
# Creates exception for words containing punctuation, "e.g." & "i.e."
data_frame$abstract <- as.character(df$abstract, na.omit = T)
data_frame <- data_frame %>%
mutate(abstract_cleaned = str_replace_all(abstract, c("e\\.g\\."="e1g1", "i\\.e\\."="i1e1")),
abstract_cleaned = str_replace_all(abstract_cleaned, c("[^a-zA-Z0-9\\&\\s]"=" ", "[\\n\\t\\f]"="")),
abstract_cleaned = str_replace_all(abstract_cleaned, c("e1g1"="e.g.", "i1e1"="i.e.")))
return(data_frame)
}
model_text <- function(documents, metadata) {
print("Processing documents")
processed <- textProcessor(documents, metadata = metadata)
# TODO experiment with `lower.thresh` arg to prepDocuments
# plotRemoved(processed$documents, lower.thresh = seq(1, 200, by = 10))
print("Preparing documents for modeling")
out <- prepDocuments(processed$documents, processed$vocab, processed$meta, lower.thresh = 10)
# TODO what kind of additional processing needs doing to sanity check the above?
# (e.g. seeing which docs were removed entirely)
return(out)
}
model_text <- function(documents, metadata) {
print("Processing documents")
processed <- textProcessor(documents, metadata = metadata)
# TODO experiment with `lower.thresh` arg to prepDocuments
# plotRemoved(processed$documents, lower.thresh = seq(1, 200, by = 10))
print("Preparing documents for modeling")
out <- prepDocuments(processed$documents, processed$vocab, processed$meta, lower.thresh = 10)
# TODO what kind of additional processing needs doing to sanity check the above?
# (e.g. seeing which docs were removed entirely)
return(out)
}
DATA = "cogsci_papers.csv"
DATA_ALT = "cogsci_abstracts.csv"
print("Reading in abstracts data")
df.abstracts <- read_csv(DATA)
print("Cleaning abstracts")
df.abstracts <- clean_abstracts(df.abstracts)
print("Generating model framework")
abstract.model.framework <- model_text(df.abstracts$abstract_cleaned, df)
# Sanity checks
length(abstract.model.framework$documents) # 7844 compare to 7871 from initial read.csv
length(abstract.model.framework$vocab)
length(abstract.model.framework$meta)
abstract.model.manual <- stm(documents = abstract.model.framework$documents,
vocab = abstract.model.framework$vocab,
K = 10,
max.em.its = 75,
init.type = "Spectral")
labelTopics(abstract.model.manual)
structure_text <- function(documents, metadata) {
print("Processing documents")
processed <- textProcessor(documents, metadata = metadata)
# TODO experiment with `lower.thresh` arg to prepDocuments
# plotRemoved(processed$documents, lower.thresh = seq(1, 200, by = 10))
print("Preparing documents for modeling")
out <- prepDocuments(processed$documents, processed$vocab, processed$meta, lower.thresh = 10)
# TODO what kind of additional processing needs doing to sanity check the above?
# (e.g. seeing which docs were removed entirely)
return(out)
}
print("Reading in abstracts data")
df.abstracts <- read_csv(DATA_ALT)
print("Cleaning abstracts")
df.abstracts <- clean_abstracts(df.abstracts)
print("Reading in abstracts data")
df.abstracts.alt <- read_csv(DATA_ALT)
print("Cleaning abstracts")
df.abstracts.alt <- clean_abstracts(df.abstracts.alt)
clean_abstracts <- function(data_frame) {
# Clean abstract column
# Removes punctuation and escape characters, "\\n", "\\t", "\\f".
# Creates exception for words containing punctuation, "e.g." & "i.e."
data_frame$abstract <- as.character(data_frame$abstract, na.omit = T)
data_frame <- data_frame %>%
mutate(abstract_cleaned = str_replace_all(abstract, c("e\\.g\\."="e1g1", "i\\.e\\."="i1e1")),
abstract_cleaned = str_replace_all(abstract_cleaned, c("[^a-zA-Z0-9\\&\\s]"=" ", "[\\n\\t\\f]"="")),
abstract_cleaned = str_replace_all(abstract_cleaned, c("e1g1"="e.g.", "i1e1"="i.e.")))
return(data_frame)
}
print("Reading in abstracts data")
df.abstracts.alt <- read_csv(DATA_ALT)
print("Cleaning abstracts")
df.abstracts.alt <- clean_abstracts(df.abstracts.alt)
print("Generating model framework")
abstract.model.framework <- structure_text(df.abstracts.alt$abstract_cleaned, df.abstracts.alt) # takes < 1 min.
# Sanity checks
length(abstract.model.framework$documents)
length(abstract.model.framework$vocab)
length(abstract.model.framework$meta)
# Fit model
abstract.model.manual <- stm(documents = abstract.model.framework$documents,
vocab = abstract.model.framework$vocab,
K = 10,
max.em.its = 75,
init.type = "Spectral") # Takes 1-2 mins.
# Validate model
labelTopics(abstract.model.manual)
print("Reading in abstracts data")
df.abstracts <- read_csv(DATA)
print("Cleaning abstracts")
df.abstracts <- clean_abstracts(df.abstracts)
print("Generating model framework")
abstract.model.framework <- structure_text(df.abstracts$abstract_cleaned, df.abstracts) # takes < 1 min.
# Sanity checks
length(abstract.model.framework$documents)
length(abstract.model.framework$vocab)
length(abstract.model.framework$meta)
# Fit model
abstract.model.manual <- stm(documents = abstract.model.framework$documents,
vocab = abstract.model.framework$vocab,
K = 10,
max.em.its = 75,
init.type = "Spectral") # Takes 1-2 mins.
# Validate model
labelTopics(abstract.model.manual)
print("Reading in full text data :O")
df.fulltext <- read_csv(DATA)
print("Reading in full text data :O")
df.fulltext <- read_csv(DATA)
print("Generating model framework")
fulltext.model.framework <- structure_text(df.fulltext$fulltext, df.fulltext) # takes 5-10 mins.
structure_text <- function(documents, metadata = NA) {
print("Processing documents")
processed <- textProcessor(documents, metadata = metadata)
# TODO experiment with `lower.thresh` arg to prepDocuments
# plotRemoved(processed$documents, lower.thresh = seq(1, 200, by = 10))
print("Preparing documents for modeling")
out <- prepDocuments(processed$documents, processed$vocab, processed$meta, lower.thresh = 10)
# TODO what kind of additional processing needs doing to sanity check the above?
# (e.g. seeing which docs were removed entirely)
return(out)
}
print("Reading in abstracts data")
df.abstracts <- read_csv(DATA)
print("Cleaning abstracts")
df.abstracts <- clean_abstracts(df.abstracts)
print("Generating model framework")
abstract.model.framework <- structure_text(df.abstracts$abstract_cleaned, df.abstracts) # takes < 1 min.
# Sanity checks
length(abstract.model.framework$documents)
length(abstract.model.framework$vocab)
length(abstract.model.framework$meta)
# Fit model
abstract.model.manual <- stm(documents = abstract.model.framework$documents,
vocab = abstract.model.framework$vocab,
K = 10,
max.em.its = 75, # Converges after ~50 iterations
init.type = "Spectral") # Takes 1-2 mins.
# Validate model
labelTopics(abstract.model.manual)
print("Reading in full text data :O")
df.fulltext <- read_csv(DATA)
print("Generating model framework")
fulltext.model.framework <- structure_text(df.fulltext$fulltext) # takes 5-10 mins. TODO why does metadata not work?
glimpse(df.fulltext)
fulltext.model.framework <- structure_text(df.fulltext$full_text) # takes 5-10 mins. TODO why does metadata not work?
structure_text <- function(documents, metadata = NA) {
print("Processing documents")
if (!is.na(metadata)) {
processed <- textProcessor(documents, metadata = metadata)
} else {
processed <- textProcessor(documents)
}
# TODO experiment with `lower.thresh` arg to prepDocuments
# plotRemoved(processed$documents, lower.thresh = seq(1, 200, by = 10))
print("Preparing documents for modeling")
out <- prepDocuments(processed$documents, processed$vocab, processed$meta, lower.thresh = 10)
# TODO what kind of additional processing needs doing to sanity check the above?
# (e.g. seeing which docs were removed entirely)
return(out)
}
print("Reading in ALT abstracts data")
df.abstracts.alt <- read_csv(DATA_ALT)
print("Cleaning abstracts")
df.abstracts.alt <- clean_abstracts(df.abstracts.alt)
print("Generating model framework")
abstract.model.framework <- structure_text(df.abstracts.alt$abstract_cleaned, df.abstracts.alt) # takes < 1 min.
structure_text <- function(documents, metadata = NA) {
print("Processing documents")
if (!is.na(metadata)) {
processed <- textProcessor(documents, metadata = metadata)
} else {
processed <- textProcessor(documents)
}
# TODO experiment with `lower.thresh` arg to prepDocuments
# plotRemoved(processed$documents, lower.thresh = seq(1, 200, by = 10))
print("Preparing documents for modeling")
out <- prepDocuments(processed$documents, processed$vocab, processed$meta, lower.thresh = 10)
# TODO what kind of additional processing needs doing to sanity check the above?
# (e.g. seeing which docs were removed entirely)
return(out)
}
print("Reading in abstracts data")
df.abstracts <- read_csv(DATA)
print("Cleaning abstracts")
df.abstracts <- clean_abstracts(df.abstracts)
print("Generating model framework")
abstract.model.framework <- structure_text(df.abstracts$abstract_cleaned, df.abstracts) # takes < 1 min.
# Sanity checks
length(abstract.model.framework$documents)
length(abstract.model.framework$vocab)
length(abstract.model.framework$meta)
print("Reading in full text data :O")
df.fulltext <- read_csv(DATA)
print("Generating model framework")
fulltext.model.framework <- structure_text(df.fulltext$full_text) # takes 5-10 mins. TODO why does metadata not work?
# Sanity checks
length(fulltext.model.framework$documents)
length(fulltext.model.framework$vocab)
length(fulltext.model.framework$meta)
# Fit model
fulltext.model.manual <- stm(documents = fulltext.model.framework$documents,
vocab = fulltext.model.framework$vocab,
K = 10,
max.em.its = 75, # Converges after
init.type = "Spectral") # Takes 1-2 mins.
# Validate model
labelTopics(fulltext.model.manual)
# Fit model
fulltext.model.manual <- stm(documents = fulltext.model.framework$documents,
vocab = fulltext.model.framework$vocab,
K = 50,
max.em.its = 75, # Converges after 33 iterations
init.type = "Spectral") # Takes up to 10 mins.
# Validate model
labelTopics(fulltext.model.manual)
# Fit model
fulltext.model.manual <- stm(documents = fulltext.model.framework$documents,
vocab = fulltext.model.framework$vocab,
K = 20,
max.em.its = 75, # Converges after 33 iterations
init.type = "Spectral") # Takes up to 10 mins.
# Validate model
labelTopics(fulltext.model.manual)
# Fit model
fulltext.model.manual <- stm(documents = fulltext.model.framework$documents,
vocab = fulltext.model.framework$vocab,
K = 25,
max.em.its = 150,
init.type = "Spectral") # Takes up to 10 mins.
# Validate model
labelTopics(fulltext.model.manual)
# Fit model
fulltext.model.manual <- stm(documents = fulltext.model.framework$documents,
vocab = fulltext.model.framework$vocab,
K = 50,
max.em.its = 150,
init.type = "Spectral") # Takes up to 10 mins.
# Validate model
labelTopics(fulltext.model.manual)
clean_abstracts <- function(data_frame) {
# Clean abstract column (expects a data frame with an `abstract` column)
# Removes punctuation and escape characters, "\\n", "\\t", "\\f".
# Creates exception for words containing punctuation, "e.g." & "i.e."
data_frame$abstract <- as.character(data_frame$abstract, na.omit = T)
data_frame <- data_frame %>%
mutate(abstract_cleaned = str_replace_all(abstract, c("e\\.g\\."="e1g1", "i\\.e\\."="i1e1")),
abstract_cleaned = str_replace_all(abstract_cleaned, c("[^a-zA-Z0-9\\&\\s]"=" ", "[\\n\\t\\f]"="")),
abstract_cleaned = str_replace_all(abstract_cleaned, c("e1g1"="e.g.", "i1e1"="i.e.")))
return(data_frame)
}
structure_text <- function(documents, metadata = NA) {
print("Processing documents")
if (!is.na(metadata)) {
processed <- textProcessor(documents, metadata = metadata)
} else {
processed <- textProcessor(documents)
}
print("Preparing documents for modeling")
out <- prepDocuments(processed$documents, processed$vocab, processed$meta, lower.thresh = 10)
return(out)
}
rm(list=ls()
)
clean_abstracts <- function(data_frame) {
# Clean abstract column (expects a data frame with an `abstract` column)
# Removes punctuation and escape characters, "\\n", "\\t", "\\f".
# Creates exception for words containing punctuation, "e.g." & "i.e."
data_frame$abstract <- as.character(data_frame$abstract, na.omit = T)
data_frame <- data_frame %>%
mutate(abstract_cleaned = str_replace_all(abstract, c("e\\.g\\."="e1g1", "i\\.e\\."="i1e1")),
abstract_cleaned = str_replace_all(abstract_cleaned, c("[^a-zA-Z0-9\\&\\s]"=" ", "[\\n\\t\\f]"="")),
abstract_cleaned = str_replace_all(abstract_cleaned, c("e1g1"="e.g.", "i1e1"="i.e.")))
return(data_frame)
}
structure_text <- function(documents, metadata = NA) {
print("Processing documents")
if (!is.na(metadata)) {
processed <- textProcessor(documents, metadata = metadata)
} else {
processed <- textProcessor(documents)
}
print("Preparing documents for modeling")
out <- prepDocuments(processed$documents, processed$vocab, processed$meta, lower.thresh = 10)
return(out)
}
DATA = "cogsci_papers.csv"
DATA_ALT = "cogsci_abstracts.csv"
# Model original abstracts data
df.abstracts.alt <- read_csv(DATA_ALT)
df.abstracts.alt <- clean_abstracts(df.abstracts.alt)
abstract.model.framework <- structure_text(df.abstracts.alt$abstract_cleaned, df.abstracts.alt) # takes < 1 min.
# Fit model
abstract.model.manual <- stm(documents = abstract.model.framework$documents,
vocab = abstract.model.framework$vocab,
K = 10,
max.em.its = 75,
init.type = "Spectral") # Takes 1-2 mins.
# Validate model
labelTopics(abstract.model.manual)
cloud(stmobj = abstract.model.manual,
topic = 1,
type = "model",
max.words = 25)
install.packages("wordcloud")
library(wordcloud)
cloud(stmobj = abstract.model.manual,
topic = 1,
type = "model",
max.words = 25)
cloud(stmobj = abstract.model.manual,
topic = 1,
type = "documents",
documents = abstract.model.framework$documents,
thresh = 0.9,
max.words = 25) # word cloud of most probable 25 words in topic 1 selected from most likely documents
cloud(stmobj = abstract.model.manual,
topic = 1,
type = "documents",
documents = abstract.model.framework$documents,
thresh = 0.8,
max.words = 25) # word cloud of most probable 25 words in topic 1 selected from most likely documents
cloud(stmobj = abstract.model.manual,
topic = 1,
type = "documents",
documents = abstract.model.framework$documents,
thresh = 0.8,
max.words = 25) # word cloud of most probable 25 words in topic 1 selected from most likely documents
# Visualize model
cloud(stmobj = abstract.model.manual,
topic = 1,
type = "model",
max.words = 25) # word cloud of most probable 25 words in topic 1
findThoughts(abstract.model.manual, texts = df.abstracts.alt$abstract_cleaned, n = 3)
findThoughts(abstract.model.manual, texts = df.abstracts.alt$abstract_cleaned, n = 3, topics = 1)
df.abstracts.alt <- read_csv(DATA_ALT)
df.abstracts.alt <- clean_abstracts(df.abstracts.alt)
processed <- textProcessor(df.abstracts.alt$abstract_cleaned, metadata = df.abstracts.alt)
removed = processed$docs.removed
length(removed)
length(df.abstracts.alt)
length(df.abstracts.alt$abstract_cleaned)
new.docs = df.abstracts.alt[-removed,]
length(new.docs)
length(new.docs$abstract_cleaned)
7871-7844
abstract.model.framework <- prepDocuments(processed$documents, processed$vocab, processed$meta, lower.thresh = 10)
# Fit model
abstract.model.manual <- stm(documents = abstract.model.framework$documents,
vocab = abstract.model.framework$vocab,
K = 10,
max.em.its = 75,
init.type = "Spectral") # Takes 1-2 mins.
# Validate model
labelTopics(abstract.model.manual)
library(wordcloud)
cloud(stmobj = abstract.model.manual,
topic = 1,
type = "model",
max.words = 25) # word cloud of most probable 25 words in topic 1
cloud(stmobj = abstract.model.manual,
topic = 1,
type = "documents",
documents = abstract.model.framework$documents,
thresh = 0.8,
max.words = 25) # word cloud of most probable 25 words in topic 1 selected from most likely documents
# View the top n documents for each topic
# TODO get this working: how to figure out which abstracts were removed during processing
topic.docs <- findThoughts(abstract.model, texts = new.docs, n = 3, topics = c(1))
# View the top n documents for each topic
# TODO get this working: how to figure out which abstracts were removed during processing
topic.docs <- findThoughts(abstract.model.manual, texts = new.docs, n = 3, topics = c(1))
# View the top n documents for each topic
# TODO get this working: how to figure out which abstracts were removed during processing
topic.docs <- findThoughts(abstract.model.manual, texts = new.docs$abstract_cleaned, n = 3, topics = c(1))
topic.docs
length(topics.docs)
length(topic.docs)
dim(topic.docs)
glimpse(topic.docs)
topic.docs[1]
topic.docs[2]
rm(list=ls())
# Model full text
df.fulltext <- read_csv(DATA)
DATA = "cogsci_papers.csv"
# Model full text
df.fulltext <- read_csv(DATA)
#  TODO why does metadata not work?
fulltext.model.framework <- structure_text(df.fulltext$full_text) # takes up to 20 mins.
structure_text <- function(documents, metadata = NA) {
print("Processing documents")
if (!is.na(metadata)) {
processed <- textProcessor(documents, metadata = metadata)
} else {
processed <- textProcessor(documents)
}
print("Preparing documents for modeling")
out <- prepDocuments(processed$documents, processed$vocab, processed$meta, lower.thresh = 10)
return(out)
}
#  TODO why does metadata not work?
fulltext.model.framework <- structure_text(df.fulltext$full_text) # takes up to 20 mins.
# Fit model
fulltext.model.manual <- stm(documents = fulltext.model.framework$documents,
vocab = fulltext.model.framework$vocab,
K = 50,
max.em.its = 100,
init.type = "Spectral") # Takes up to 10 mins.
# Validate model
labelTopics(fulltext.model.manual)
# Graph model
cloud(stmobj = fulltext.model.manual,
topic = 2,
type = "model",
max.words = 25) # word cloud of most probable 25 words in topic 1
# Graph model
cloud(stmobj = fulltext.model.manual,
topic = 45,
type = "model",
max.words = 25) # word cloud of most probable 25 words in topic 1
# Graph model
cloud(stmobj = fulltext.model.manual,
topic = 37,
type = "model",
max.words = 25) # word cloud of most probable 25 words in topic 1
# Graph model
cloud(stmobj = fulltext.model.manual,
topic = 22,
type = "model",
max.words = 25) # word cloud of most probable 25 words in topic 1
